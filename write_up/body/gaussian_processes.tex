\chapter{Gaussian Processes}

\begin{itemize}
    \item Prove all pos-sem is cov
    \item prove exponential quad kern is pos semi-def
\end{itemize}

\section{Motivation and Definitions}

If we knew the distribution of $D(\bm{\theta})$
exactly, then in order to do approximate
Bayesian computation, running the model becomes superfluous to
obtain a `sample' from $D(\bm{\theta})$. Instead a sample $D(\bm{\theta})$ 
could be drawn directly from it's distribution. It is highly improbable that
the distribution of $D(\bm{\theta})$ is known in practice, and so
this chapter describes a method of approximating the distribution. 
Furthermore, since $\Pr(D(\bm{\theta}) < \varepsilon)$
is approximately
proportional to the true likelihood, sampling from the approximation of 
$D(\bm{\theta})$ can be used to for more efficient approximation of 
the likelihood. The approximation considered is achieved by modelling
$D(\bm\theta)$ 
as a realisation of a Gaussian process.

\begin{definition}[Gaussian Process]\label{def:gp}
    A collection of random variables $\{f(x)\}_{x\in\mathcal{X}}$
    (where $x$ may be a vector) is a \emph{Gaussian process} if any finite
    subset of the collection of random variables is multivariate normal
    distributed. That is, there is a function $m:\mathcal{X}\to\R$ and
    symmetric kernel $k:\mathcal{X}\times\mathcal{X}\to \R$ such that for all
    finite sets
    $\mathbf{x} :=\{x_1, x_2, \dots, x_n\} \subset \mathcal{J},$ with
    $f(\mathbf{x}) := [f(x_1), f(x_2), \dots, f(x_n)]^T$
    $$f(\mathbf{x}) \sim
        \MVN\left(\begin{bmatrix}
            m(x_1) \\ m(x_2)\\ \vdots\\ m(x_n)
        \end{bmatrix},\, \mathbf{K} = \begin{bmatrix}
            k(x_1, x_1) & k(x_1, x_2) & \dots  & k(x_1, x_n) \\
            k(x_2, x_1) & \ddots      &        & \vdots      \\
            \vdots      &             & \ddots & \vdots      \\
            k(x_n, x_1) & \cdots      & \cdots & k(x_n, x_n)
        \end{bmatrix}\right).$$
\end{definition}

\begin{definition}[Mean and Covariance Function]\label{def:mean_kernel}
    The the \emph{mean function} and \emph{covariance kernel} are
    $$m(x_i) := \E\left[f(x_i)\right]$$ and
    $$k(x_{i}, x_{i^\prime}) := \cov\left(f(x_i), f(x_{i^\prime})\right).$$
\end{definition}

Although Gaussian processes are simultaneously realised over the whole space
$\mathcal{X}$ (for example $\R^d$) and are hence collections of 
(uncountably infinite)
random variables, the choice of covariance function
$\mathrm{corr}(x, x^\prime) \to 1$ as $||x - x^\prime||\to 0$
induces continuity in $x$ almost
surely. Therefore they can be thought of as realisations of continuous 
functions.
\textcolor{red}{(Should I prove this?)}

Some common examples of Gaussian processes include \begin{enumerate}
    \item Brownian motion on $\R$:
          $$m\equiv 0, \quad \text{and}\quad k(s, t) = \min(s, t)$$
    \item Ornstein Uhlenbeck process with parameters $\theta$ and $\sigma$:
          $$
              m\equiv 0, \quad \text{and}
              \quad k(s, t)
              = \frac{\sigma^2_k}{2\theta} \left(
              e^{-\theta|t - s|} - e^{-\theta(t + s)}
              \right)
          $$
\end{enumerate}

% \begin{definition}[Brownian Motion]
%     $B(t):\R^+ \to \R$ is a \emph{Brownian motion} on $\R$ if\begin{enumerate}
%         \item $B(0) = 0$ almost surely
%         \item $B(t_0), B(t_1) - B(t_0), \dots, B(t_n - t_{n-1})$ are
%               independent for all $t_0<t_1<t_2<\dots<t_n$
%         \item $B(t + s) - B(t)\sim N(0, s)$ for $s, t \geq 0$
%         \item $B(t)$ is continuous almost surely for $t>0.$
%     \end{enumerate}
% \end{definition}

Properties such as the smoothness and undulation of the
realised functions are also determined by the
covariance kernel $k,$ and associated hyperparameters. Before exploring
different kernel options, we begin by
\textcolor{red}{defining a valid kernel function, and} formalising `smoothness.'

\color{red}

\begin{definition}[Positive Semi-Definite Matrix]\label{def:pos_def_mat}
    An $n\times n$ matrix $\mathbf{A}$ is \emph{positive semi-definite} if
    $\mathbf{v}^T\mathbf{A}\mathbf{v} \geq 0$ for all $\mathbf{v}\in\R^n.$
\end{definition}

\begin{theorem}[Sufficient Condition for Positive Semi-Definite]
    A symmetric matrix $\mathbf{A}$ is positive semi-definite, if (and only if)
    it's eigenvalues are non-negative.
\end{theorem}
\begin{proof}
    % https://en.wikipedia.org/wiki/Definite_matrix#Eigenvalues
\end{proof}

\begin{definition}[Positive Semi-Definite Kernel]\label{def:pos_def_ker}
    A kernel $k:\mathcal{X}\times\mathcal{X}\to\R$ is
    \emph{positive semi-definite} if the matrix
    $$\mathbf{K} = \begin{bmatrix}
            k(x_1, x_1) & k(x_1, x_2) & \dots  & k(x_1, x_n) \\
            k(x_2, x_1) & \ddots      &        & \vdots      \\
            \vdots      &             & \ddots & \vdots      \\
            k(x_n, x_1) & \cdots      & \cdots & k(x_n, x_n)
        \end{bmatrix}$$
    is positive semi-definite for any collection of $x_i\in\mathcal{X}$
\end{definition}

\begin{theorem}
    All symmetric positive semi-definite matrices are covariance matrices for
    some set of random variables
\end{theorem}
\begin{proof}
    % https://www.fepress.org/wp-content/uploads/2014/06/ch7-iff-covariance_correlation_matrix.pdf
\end{proof}

\color{black}

\begin{definition}[Mean Square Continuous]
    A function $f:\R^d\to\R$ is \emph{mean square continuous} at $\mathbf{x}$
    in the $i$th direction at if
    $\E(|f(\mathbf{x} + h\mathbf{e}_i) - f(\mathbf{x})|^2)\to 0$ as $|h|\to 0,$
    where $\mathbf{e}_i$ is the unit vector with a 1 in the $i$th coordinate.
\end{definition}

\begin{definition}[Mean Square Differentiable]
    A function $f:\R^d\to\R$ is \emph{mean square differentiable} at
    $\mathbf{x}$ in the $i$th direction with derivative
    $\frac{\partial f(\mathbf{x})}{\partial x_i}$ if
    $$
        \E\left[
            \left|
            \frac{f(\mathbf{x} + h\mathbf{e}_i) - f(\mathbf{x})}{h}
            - \frac{\partial f(\mathbf{x})}{\partial x_i}
            \right|^2
            \right]\to 0
    $$ as $|h|\to 0,$ where $\mathbf{e}_i$ is the unit vector in the direction
    of the $i$th coordinate.
\end{definition}

The concept of mean square differentiability and continuity are analogous to
differentiability and continuity in the non-random function case.

\begin{theorem}
    Brownian motion is mean square continuous, but not mean square
    differentiable.
\end{theorem}
\begin{proof}
    $(B_{t + h} - B_t)^2 \sim (\sqrt{|h|}Z)^2$ where $Z\sim N(0,1).$ Therefore
    $(B_{t + h} - B_t)^2 \sim |h|\chi_1^2 \to 0$ almost surely as $|h|\to 0,$
    and hence $\E[(B_{t + h} - B_t)^2= 0]$. Since
    $\frac{B_{t + h} - B_t}{h} \sim N(0, 1/|h|),$ $\frac{B_{t + h} - B_t}{h}$
    does not converge to any valid probability distribution as $|h| \to 0,$ as
    the variance approaches $+\infty.$
\end{proof}

Some common

\color{red}

\begin{theorem}[Positive Semi-Definiteness of RBF]\label{thm:rbf_pos_def}
    The radial basis function
    ${\sigma^2_k}\exp(-\frac{(x - x^\prime)^2}{2\gamma^2})$ is a positive
    semi-definite kernel.
\end{theorem}

\begin{theorem}[Bochner's Theorem]
    Let $k$ be a stationary kernel function such that
    $k(x, x^\prime) = f(d)$. A function $k:\R^d\to\mathbb{C}$ is the covariance
    function of a weakly
    stationary mean square continuous complex-valued random process of $\R^d$
    if and only if it can be represented as
    $$k(\mathbf{\tau}) = \int_{\R^d} \exp(2\pi i \mathbf{s}\cdot\mathbf{\tau})$$
\end{theorem}

\parencite[82]{rasmussen_gaussian_2008}

\color{black}

\section{Families of Kernel Function}

The two most common families of kernel functions are the squared exponential
and Mat\'ern families.

\subsection*{Mat\'{e}rn Family}

The Mat\'ern exponential kernel is of the form
$$k_\nu(x, x^\prime)
    = \sigma^2_k \frac{2^{1 - \nu}}{\Gamma(\nu)}
    \left(\frac{\sqrt{2\nu}||x - x^\prime||}{\ell}\right)^\nu
    K_\nu\left(-\frac{\sqrt{2\nu}||x - x^\prime||}{\ell}\right)$$
where $K_\nu$ is a modified Bessel function
(defined in \cite[374]{abramowitz_handbook_2013}). The general form is not very
insightful, however for $\nu = 1/2, 3/2$ and $5/2,$ (the most common values 
used) the kernel
can be written as:
$$
    k_{1/2}(x, x^\prime)
    = \sigma^2_k\exp\left(-\frac{||x - x^\prime||}{\ell}\right)
$$
$$
    k_{3/2}(x, x^\prime)
    = \sigma^2_k
    \left(1 + \frac{\sqrt{3}||x - x^\prime||}{\ell}\right)
    \exp\left(-\frac{\sqrt{3}||x - x^\prime||}{\ell}\right)
$$
$$
    k_{5/2}(x, x^\prime)
    = \sigma^2_k
    \left(
    1 + \frac{\sqrt{5}||x - x^\prime||}{\ell} + \frac{5||x - x^\prime||}{3\ell^2}
    \right)
    \exp\left(-\frac{||x - x^\prime||^2}{2*\ell^2}\right)
$$

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{maternonehalf_kernel.pdf}
        \subcaption{Mat\'ern 1/2 Kernel}
        \label{fig:mat_one_half}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{maternthreehalves_kernel.pdf}
        \subcaption{Mat\'ern 3/2 Kernel}
        \label{fig:mat_three_halves}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{maternfivehalves_kernel.pdf}
        \subcaption{Mat\'ern 5/2 Kernel}
        \label{fig:mat_five_halves}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{exponentiated_kernel.pdf}
        \subcaption{Squared Exponential Kernel}
        \label{fig:squared_exp}
    \end{subfigure}%
    \caption{
        Ten sample realisations from 4 different kernels with hyperparameters
        $\ell = 1,$ and $\sigma^2_o = 1.$ One realisation is bolded. Samples
        for each kernel were generated from the same seed.
    }
    \label{fig:kernels}
\end{figure}

Zero mean Gaussian processes with a Mat\'ern kernel are $n$ times mean square
differentiable, for all $n < \nu.$ As seen in Figure \ref{fig:kernels}, this
means that this kernel allows for flexibility in how smooth realised functions
are.
As $\nu\to\infty,$ with appropriate
rescaling, the limit of the Mat\'ern kernel is the squared exponential
kernel.\cite[85]{rasmussen_gaussian_2008}
\textcolor{red}{Proof in CHAPTER 4 SKOROKHOD STOCHASTIC I?}

\subsection*{Squared Exponential Kernel}

The squared exponential kernel is of the form
$$
    k(x, x^\prime)
    = \sigma^2_k\exp\left(-\frac{||x - x^\prime||^2}{2\ell^2}\right)
$$

As the limit of Mat\'ern kernels, the squared exponential kernel is infinitely
mean square differentiable. Despite this being the `default' kernel in much of
the literature, infinite differentiability is a very strong condition on
functions which are very smooth, which can be seen in Figure
\ref{fig:squared_exp}

\subsection*{Length and Amplitude Hyperparameters}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{GP_ell_5_sigma2_5_tenths.pdf}
        \subcaption{$\ell = \sigma^2_k = 1/2$}
        \label{fig:half_half}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{GP_ell_5_sigma2_20_tenths.pdf}
        \subcaption{$\ell = 1/2, \sigma^2_k = 2$}
        \label{fig:half_two}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{GP_ell_20_sigma2_5_tenths.pdf}
        \subcaption{$\ell = 2, \sigma^2_k = 1/2$}
        \label{fig:two_half}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{GP_ell_20_sigma2_20_tenths.pdf}
        \subcaption{$\ell = \sigma^2_k = 2$}
        \label{fig:two_two}
    \end{subfigure}%
    \caption{
        Ten realisations of zero mean Gaussian processes with the squared
        exponential kernel, varying the length and amplitude parameters.
        The samples were generated using the same seed
    }
    \label{fig:length_amp}
\end{figure}

In both the Mat\'ern and squared quadratic kernels
(as well as most other common kernels choices), there are two
hyperparameters $\ell$ and $\sigma^2_k$ which are referred to as length and
amplitude hyperparameters. $\ell$ determines how close two points need
to be to be highly correlated. Larger values of $\ell$ generates functions with
higher correlation within a larger neighbourhood,
as seen in Figure \ref{fig:length_amp}. $\sigma^2_k$ does not impact
the correlation between $x$ and $x^\prime,$ but scales the correlation
matrix. In other words, larger $\sigma^2_k$ increase the size but not rate of
fluctuations. This can be seen comparing Figure \ref{fig:half_half} to Figure
\ref{fig:half_two}.

\chapter{Gaussian Process Regression}

Given the set of observations $f(\mathbf{x}_*)$ for the set of indices
$\mathbf{x}_*,$ it is
often desirable to infer information about the function values at
unobserved values. By choosing a Gaussian process with fixed kernel and
hyperparameters, we
can condition the process on the observed data, limiting the family of
possible functions that we assume truly describe the model.
Under the assumption that the function is a realisation of a Gaussian
process predicting unseen function values reduces to elementary linear algebra.
This is because a conditional multivariate normal distribution
is still multivariate normal, and so the distribution of unobserved points will
be multivariate normal.

Consider
$$
    \begin{bmatrix}
        f(\mathbf{x}) \\
        f(\mathbf{x}_*)
    \end{bmatrix} \sim \mathcal{N}\left(
    \begin{bmatrix}
            m(\mathbf{x}) \\
            m(\mathbf{x}_*)
        \end{bmatrix}, \begin{bmatrix}
            K       & K_{*}  \\
            K_{*}^T & K_{**}
        \end{bmatrix}
    \right)
$$

\begin{theorem}[Conditional Multivariate Normal Distribution is Multivariate Normal]
    \label{thm:cond_mvn}

    With $f(\mathbf{x})$ and $f(\mathbf{x}_*),$ the conditional distribution is
    $$
        f(\mathbf{x}) | f(\mathbf{x}_*)
        \sim \mathcal{N}\left(
        m(\mathbf{x}) + K_{*}K_{**}^{-1}(f(\mathbf{x}_*) - m(\mathbf{x}_*)), \,\,
        K - K_{*}K_{**}^{-1}K_{*}^T
        \right).
    $$
\end{theorem}
\begin{proof}
    % https://statproofbook.github.io/P/mvn-cond.html
    Since marginal distribution of the multivariate normal distribution, is also
    multivariate normal,
    $f(\mathbf{x}_*) \sim \mathcal{N}(m(\mathbf{x}_*), K).$
    Let the inverse of $\begin{bmatrix}
            K       & K_{*}  \\
            K_{*}^T & K_{**}
        \end{bmatrix}$ be defined as
    $$
        \begin{bmatrix}
            \tilde{K}       & \tilde{K}_{*}  \\
            \tilde{K}_{*}^T & \tilde{K}_{**}
        \end{bmatrix} =
        \begin{bmatrix}
            (K - K_{*} K_{**}^{-1} K_{*}^T)^{-1}
             & -(K
            - K_{*} K_{**}^{-1} K_{*}^T)^{-1} K_{*} K_{**}^{-1} \\
            -K_{**}^{-1} K_{*}^T (K
            - K_{*} K_{**}^{-1} K_{*}^T)^{-1}
             & K_{**}^{-1} + K_{**}^{-1} K_{*}^T (K
            - K_{*} K_{**}^{-1} K_{*}^T)^{-1} K_{*} K_{**}^{-1}
        \end{bmatrix}
    $$ by the inverse of a block matrix. Therefore
    \begin{align*}
        p(f(\mathbf{x}) | f(\mathbf{x}_*))
        =       & \frac{p(f(\mathbf{x}), f(\mathbf{x}_*))}{p(f(\mathbf{x}_*))} \\
        \propto & \frac{\exp \left[ -\frac{1}{2} \left(\begin{bmatrix}
                                                               f(\mathbf{x}) \\
                                                               f(\mathbf{x}_*)
                                                           \end{bmatrix} -
        \begin{bmatrix}
                m(\mathbf{x}) \\
                m(\mathbf{x}_*)
            \end{bmatrix}
        \right)^\mathrm{T} \begin{bmatrix}
                               K       & K_{*} \\
                               K_{*}^T & K
                           \end{bmatrix}^{-1} \left(
        \begin{bmatrix}
                f(\mathbf{x}) \\
                f(\mathbf{x}_*)
            \end{bmatrix}-
        \begin{bmatrix}
                m(\mathbf{x}) \\
                m(\mathbf{x}_*)
            \end{bmatrix}\right) \right]}
        {
        \exp \left[
        -\frac{1}{2}
        (f(\mathbf{x}_*)-m(\mathbf{x}_*))^\mathrm{T}
        K_{**}^{-1}
        (f(\mathbf{x}_*)-m(\mathbf{x}_*))
        \right]
        }                                                                      \\
        =       & \exp \left[ -\frac{1}{2} \left(\begin{bmatrix}
                                                         f(\mathbf{x}) \\
                                                         f(\mathbf{x}_*)
                                                     \end{bmatrix} -
        \begin{bmatrix}
                m(\mathbf{x}) \\
                m(\mathbf{x}_*)
            \end{bmatrix}
        \right)^\mathrm{T} \begin{bmatrix}
                               K       & K_{*}  \\
                               K_{*}^T & K_{**}
                           \end{bmatrix}^{-1} \left(
        \begin{bmatrix}
                f(\mathbf{x}) \\
                f(\mathbf{x}_*)
            \end{bmatrix} -
        \begin{bmatrix}
                m(\mathbf{x}) \\
                m(\mathbf{x}_*)
            \end{bmatrix} \right)\right.                                           \\
                & \hphantom{\exp \left[\right.} \left.
        + \frac{1}{2}(f(\mathbf{x}_*)-m(\mathbf{x}_*))^\mathrm{T}
        K_{**}^{-1}
        (f(\mathbf{x}_*)-m(\mathbf{x}_*))\right]                               \\
        =       & \exp \left[
        -\frac{1}{2} \left(
        (f(\mathbf{x})-m(\mathbf{x}))^\mathrm{T}
        \tilde{K}
        (f(\mathbf{x})-m(\mathbf{x}))\right.\right.                            \\
                & \hphantom{\exp \left[
        -\frac{1}{2} \left( \right.\right.}
        + 2 (f(\mathbf{x})-m(\mathbf{x}))^\mathrm{T}
        \tilde{K}_{*}
        (f(\mathbf{x}_*)-m(\mathbf{x}_*))                                      \\
                & \hphantom{\exp \left[
        -\frac{1}{2} \left( \right.\right.}
        \left.
        + (f(\mathbf{x}_*)-m(\mathbf{x}_*))^\mathrm{T}
        \tilde{K}_{**}
        (f(\mathbf{x}_*)-m(\mathbf{x}_*))
        \right)
        \\
                & \hphantom{\exp \left[
        -\frac{1}{2} \left( \right.\right.}\left.
        + \frac{1}{2} (f(\mathbf{x}_*)-m(\mathbf{x}_*))^\mathrm{T}
        K_{**}^{-1}
        (f(\mathbf{x}_*)-m(\mathbf{x}_*))
        \right]                                                                \\
        \propto & \exp \left[
        -\frac{1}{2}
        (f(\mathbf{x})-m(\mathbf{x}))^\mathrm{T}
        \tilde{K}
        (f(\mathbf{x})-m(\mathbf{x}))\right.                                   \\
                & \hphantom{\exp \left[
        -\frac{1}{2} \left( \right.\right.}
        -\left.
        (f(\mathbf{x})-m(\mathbf{x}))^\mathrm{T}
        \tilde{K}_{*}
        (f(\mathbf{x}_*)-m(\mathbf{x}_*))
        \right] \tag{by removing the terms independent of $f(\mathbf{x})$}.
    \end{align*}
    Since
    $$
        p(\mathbf{z})
        \propto \exp\left(
        -\frac{1}{2}\mathbf{z}^T\Sigma^{-1}\mathbf{z}
        + \mathbf{z}^T\mathbf{c}
        \right)
        \implies \mathbf{z} \sim \mathcal{N}\left(\Sigma \mathbf{c}, \Sigma\right),
    $$
    $f(\mathbf{x}) - m(\mathbf{x}) | f(\mathbf{x}_*)$ is
    multivariate normal with mean
    \begin{align*}
        - \tilde{K}^{-1}\tilde{K}_{*}(f(\mathbf{x}_*) - m(\mathbf{x}_*))
        = & (K - K_{*} K_{**}^{-1} K_{*}^T)                              \\
          & \times(K - K_{*} K_{**}^{-1} K_{*}^T)^{-1} K_{*} K_{**}^{-1}
        (f(\mathbf{x}_*) - m(\mathbf{x}_*))                              \\
        = & K_{*} K_{**}^{-1}(f(\mathbf{x}_*) - m(\mathbf{x}_*))
    \end{align*}
    and covariance matrix
    $$
        \tilde{K}^{-1} = K - K_{*} K_{**}^{-1} K_{*}^T
    $$
    by the alternative parametrisation of the multivariate normal distribution
    as a member of the exponential family of distributions
    (see \cite[Table of Distributions]{noauthor_exponential_2024}).
    Finally, by the linearity of the multivariate normal mean,
    $$
        f(\mathbf{x}) | f(\mathbf{x}_*)
        \sim \mathcal{N}\left(
        m(\mathbf{x}) + K_{*}K_{**}^{-1}(f(\mathbf{x}_*) - m(\mathbf{x}_*)), \,\,
        K - K_{*}K_{**}^{-1}K_{*}^T
        \right).
    $$
\end{proof}

After observing the function at multiple indices, we update the
predictive distribution of any unobserved points, and generate new paths. The
more points that the Gaussian process is conditioned on, the more narrow the
sample paths, as seen in Figure \ref{fig:no_var_cub_reg}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{cub_GP_1_iters.pdf}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{cub_GP_2_iters.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{cub_GP_4_iters.pdf}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{cub_GP_8_iters.pdf}
    \end{subfigure}%
    \caption{
        Sequence of Gaussian process regressions on the target function
        (black) $f(x) = x(x-1)(x+1),$ after 1, 2, 4, and 8 observations in
        blue. The red lines are new realisations from the conditioned Gaussian
        process. The Gaussian process was zero mean and had a squared
        exponential kernel. The hyperparameters were fixed with $\ell = 2.7$
        and $\sigma^2_k = 1.1$
    }
    \label{fig:no_var_cub_reg}
\end{figure}

\section{Observation Variance}

For most functions, model outputs, or processes desirable for
approximating through Gaussian process regression, multiple
observations (through model runs or an real life measurements) of the same point
will result in different observations. This is in contrast to exact realisations
as in Figure \ref{fig:no_var_cub_reg}. The simplest assumption is that the
observations are of the form
$$
    f_o(\mathbf{x}_*) = f(\mathbf{x}_*) + \bm{\varepsilon}
$$
where $\bm{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \sigma^2_o I).$
Under these assumptions,
$\mathrm{Cov}(f_o(\mathbf{x}_*), f_o(\mathbf{x}_*)) = K_{**} + \sigma^2_o I,$
where $K_{**} = \mathrm{Cov}(f(\mathbf{x}_*), f(\mathbf{x}_*))$
matrix of $f(\mathbf{x}_*)$ without noise. Therefore the conditional
distribution of our unobserved function outputs given noisy observations
$$
    f(\mathbf{x}) | f_o(\mathbf{x}_*)
    \sim \mathcal{N}\left(
    m(\mathbf{x}) + K_{*}
    (K_{**} + \sigma^2_o I)^{-1}
    (f(\mathbf{x}_*) - m(\mathbf{x}_*)), \,\,
    K - K_{*}(K_{**} + \sigma^2_o I)^{-1}K_{*}^T
    \right).
$$


\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{cub_GP_err_2_iters.pdf}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{cub_GP_err_4_iters.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{cub_GP_err_8_iters.pdf}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{cub_GP_err_16_iters.pdf}
    \end{subfigure}%
    \caption{
        Sequence of Gaussian process regressions on the target function
        (black) $f(x) = x(x-1)(x+1),$ after 2, 4, 8, and 16 observations of
        $f(x_i) + \varepsilon_i,$ where $\varepsilon_i$ is i.i.d.\
        $\mathcal{N}(0, \sigma^2_o)$ with $\sigma^2_o = 0.01$
        in blue. The red lines are new realisations from the conditioned
        Gaussian process. The Gaussian process was 0 mean and had a squared
        exponential kernel. The hyperparameters were fixed with $\ell = 2.7$
        and $\sigma^2_k = 1.1$
    }
    \label{fig:var_cub_reg}
\end{figure}

Adding noise to the observations makes the predictive distributions means
that a single observation gives less information about the underlying function,
and hence the predictive distributions of unseen data are
much less stable, as empirically seen in Figure \ref{fig:var_cub_reg}.

\section{Model Selection}

\subsection*{Kernel Family}

The appropriate choice of kernel will depend on the properties behaviour of the
target function to approximate. In the case of estimating an extremely
stochastic distribution (such as the price of a stock over time), it is
unlikely to be smooth, so no mean square differentiability is required, and a
Mat\'ern 1/2 kernel would be appropriate. If it is known that our target
function is extremely smooth, such as a finite sum of infinitely differentiable
functions (such as polynomials, sin, cos etc.) then the choice of squared
exponential kernel is the most appropriate kernel. Realistically, the
smoothness of the function will not be known a priori, and hence some sort of
compromise (such as Mat\'ern 5/2) kernel allows for flexibility.

Many other kernels exist that induce varying behaviours, such as periodic
kernels \textcolor{red}{find periodic kernel}, and non-stationary kernels
(where the covariance is dependent on $x$ and $x^\prime$, not just
$|x - x^\prime|$)
\textcolor{red}{have i defined stationary?}.

\subsection*{Hyperparameters}

The hyperparameters $\ell$ and $\sigma^2_k$ for a choice of kernel have to be
are not known beforehand, unless the function is actually a realisation from
a Gaussian process. Similarly, the observation variance $\sigma^2_o$
hyperparameter may not be a priori
known. There are two main (frequentist) ways to fit these hyperparameters:
maximum likelihood estimation, and leave-one-out cross validation.

Defining the likelihood
$
    \mathcal{L}(\ell, \sigma^2_k, \sigma^2_o)
    := p(f(\mathbf{x}_*) | \ell, \sigma^2_k, \sigma^2_o)
$ in the usual way,
the maximum likelihood estimates are
$$
    \{\hat{\ell}, \hat{\sigma}^2_k, \hat{\sigma}^2_o\} :=
    \argmax_{\{\ell, \sigma^2_k, \sigma^2_o\}}
    \mathcal{L}(\ell, \sigma^2_k, \sigma^2_o)
$$
which is equivalent to minimising
$$
    -\ln(\mathcal{L}) =\frac{1}{2} \left[
    \ln (|K_{**}(\ell, \sigma^2_k) + \sigma^2_o|\,)
    + (f(\mathbf{x}_*) - m(\mathbf{x}_*))^T
    (K_{**}(\ell, \sigma^2_k) + \sigma^2_o)^{-1}
    (f(\mathbf{x}_*) - m(\mathbf{x}_*)) + c
    \right].
$$
The covariance matrix generated by the choice of kernel $K_{**}$ is
explicitly written with its dependence on $\ell$ and $\sigma^2_k.$ $c$ is a
constant.

Leave-one-out cross validation aims to maximise the predictive log
probability.
$$
    \{\tilde{\ell}, \tilde{\sigma}^2_k, \tilde{\sigma}^2_o\}
    := \argmax_{\ell, \sigma^2_k, \sigma^2_o} \sum_{i}
    \ln p(f_i(\mathbf{x}_*) | f_{-i}(\mathbf{x}_*), \ell, \sigma^2_k, \sigma^2_o),
$$ where $f_i(\mathbf{x}_*) | f_{-i}(\mathbf{x}_*)$ is the distribution of the
$i$th element of $f(\mathbf{x}_*)$ conditioned on the rest of the
observed data excluding that element (represented by $f_{-i}(\mathbf{x}_*)$).
$f_i(\mathbf{x}_*) | f_{-i}(\mathbf{x}_*)$ can be found by
Theorem \ref{thm:cond_mvn}. Computationally efficient methods for calculating
the predictive log probability that avoid having to invert the covariance
matrix for every summand element exist. In particular it can be shown that
$f_i(\mathbf{x}_*) | f_{-i}(\mathbf{x}_*)$ has mean
$$
    f_i(\mathbf{x}_*) - m_i(\mathbf{x}_*)
    - [(K_{**} + \sigma^2_o I)^{-1}(f(\mathbf{x}_*) - m(\mathbf{x}_*))]_i
    /[(K_{**} + \sigma^2_o I)^{-1}]_{ii}
$$
and variance $1/[(K_{**} + \sigma^2_o I)^{-1}]_{ii},$ where both the mean and
covariance are (surprisingly) independent of $f_i(\mathbf{x}_*)$
\parencite{rasmussen_gaussian_2008}.

Both methods can be extended to include estimating hyperparameters given a family
of possible mean functions. For example \cite{gutmann_bayesian_2016} use
maximimum likelihood estimates for the amplitude and turning points of the
quadratic mean functions.

Recent work has shown that at least under specific conditions, the
leave-one-out estimates for the scale hyperparameter are more robust to a larger
family of target functions \parencite{naslidnyk_comparing_2024}, and the
broader literature seems to favor leave-one-out cross validation.

Finally there is scope for a Bayesian approach to model selection. By setting
priors on the hyperparameters to be estimated and using the likelihood as described
in the maximum likelihood estimation approach, a posterior distribution can be
easily contrived. A set of samples from this posterior could then be taken, or
a less prinicipled maximum a posteriori probability
estimate could then be taken for a point estimate of the hyperparameters. This
approach has some obvious benefits, particularly when taking a posterior
sample. Most obviously is that since multiple values for each hyperparameter are
sampled, the set of functions after

\section{Differing mean functions}

\begin{figure}[htbp]
    \missingfigure{GP mean functions}
\end{figure}

\section{Bayesian Acquisition Functions}

Under the assumptions that making observations from the underlying function is
costly, and we care about regions of the function with high (or low)
values, new observations should be taken where there is high probability the
function will be low. There also needs to be a trade-off between observing from
areas with high predictive mean, and high predictive variance. These ideas are
formalised by Bayesian acquisition functions $\mathcal{A}(x),$ with larger 
values corresponding to a higher `desirability.' The target function is then 
sampled at the $x$ which maximises this aquisition function. The new 
observation is then incorporated into the acquisition function.

\subsection*{Upper Confidence Bound}

The upper confidence bound is one common way of exploring this trade off. The 
upper confidence bound
$$
    \mathcal{A}_\text{UCB}(x)
    := \E[f(x) | f(\mathbf{x}_*)]
    + \eta_t \sqrt{\mathrm{Var}[f(x) | f(\mathbf{x}_*)]}
$$

Similarly we can also define 

\begin{itemize}
    \item BOLFI paper uses $$\mu(\bm\theta) - \eta_t\sqrt{\mathrm{v}(\bm\theta)}$$ \begin{itemize}
              \item $\eta_t:= \sqrt{c + 2\ln(t^{d/2 + 2})},$ and $c$ can be chosen
              \item $\mu(\bm\theta)$ and $\mathrm{v}(\bm\theta)$ are the posterior mean and variance
          \end{itemize}

    \item Could use expected information
          $$
              (\mu_\text{min} - \mu(\bm\theta)) \varPhi \left(
              \frac{\mu_\text{min} - \mu(\bm\theta)}{\sqrt{\mathrm{v}(\bm\theta)}}
              \right)
              + \sqrt{\mathrm{v}(\bm\theta)}\phi\left(
              \frac{\mu_\text{min} - \mu(\bm\theta)}{\sqrt{\mathrm{v}(\bm\theta)}}
              \right)
          $$
          \begin{itemize}
              \item $\mu_\text{min} := \min_{\bm{\theta}} \mu(\bm\theta)$
              \item $\varPhi, \phi$ CDF and PDF of standard normal
          \end{itemize}
\end{itemize}
exploration parameter proven by \cite{srinivas_gaussian_2010}