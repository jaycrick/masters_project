\chapter{Gaussian Processes}

\begin{itemize}
    \item Prove all pos-sem is cov
    \item expand on intro to motive the section
    \item prove exponential quad kern is pos semi-def
\end{itemize}

\section{Motivation and Preliminaries}

If we knew the distribution of the discrepency function described in the previous chapter across the parameter space, then in order to do approximate Bayesian computation, we would not need to actually run the model in order to obtain a `sample.' Instead we could draw from the distribution of the discrepency function at fixed $\bm\theta.$ distribution itself. Further we could create an approximate likelihood $\LABC(\bm\theta)$ and use this directly in sampling from the posterior.

This motivates methods of function approximation. To approximate our discrepency, we consider the class of plausible functions, and assign some probability to each of them. Each model simulation gives some more information that we can use to update the probability of each function being the `true' function. We will consider the classes of functions generated by Gaussian processes.

\color{red}
\begin{definition}[Kernel Function (symmetric, stationary)]\label{def:kernel}
    Any function $k:\mathcal{X}\times\mathcal{X}\to\R$ is a
    \emph{kernel function}. If $k(x, x^\prime) = k(x^\prime, x),$ the kernel is
    called \emph{symmetric}.
    % The kernel is called \emph{stationary} if the
    % $k(x, x^\prime) = f(x - x^\prime)$ for some function $f$ and metric
    % $||\cdot||.$
\end{definition}

I think I define kernels above?

\color{black}

\begin{definition}[Gaussian Process]\label{def:gp}
    A collection of random variables $\{f(x)\}_{x\in\mathcal{X}}$
    (where $x$ may be a vector) is a \emph{Gaussian process} if any finite
    subset of the collection of random variables is multivariate normal
    distributed. That is, there is a function $m:\mathcal{X}\to\R$ and
    symmetric kernel $k:\mathcal{X}\times\mathcal{X}\to \R$ such that for all finite sets
    $\mathbf{x} :=\{x_1, x_2, \dots, x_n\} \subset \mathcal{J},$ with
    $f(\mathbf{x}) := [f(x_1), f(x_2), \dots, f(x_n)]^T$
    $$f(\mathbf{x}) \sim
        \MVN\left(\begin{bmatrix}
            m(x_1) \\ m(x_2)\\ \vdots\\ m(x_n)
        \end{bmatrix},\, \mathbf{K} = \begin{bmatrix}
            k(x_1, x_1) & k(x_1, x_2) & \dots  & k(x_1, x_n) \\
            k(x_2, x_1) & \ddots      &        & \vdots      \\
            \vdots      &             & \ddots & \vdots      \\
            k(x_n, x_1) & \cdots      & \cdots & k(x_n, x_n)
        \end{bmatrix}\right).$$
\end{definition}

\begin{definition}[Mean and Covariance Function]\label{def:mean_kernel}
    We define the \emph{mean function} and \emph{covariance kernels} as
    $$m(x_i) := \E\left[f(x_i)\right]$$ and
    $$k(x_{i}, x_{i^\prime}) := \cov\left(f(x_i), f(x_{i^\prime})\right).$$
\end{definition}

Some common examples of Gaussian processes include \begin{itemize}
    \item Brownian motion on $\R$: $$m\equiv 0,$$ and $$k(s, t) = \min(s, t)$$
    \item Ornstein Uhlenbeck process with parameters $\theta$ and $\sigma$:
          $$m\equiv 0,$$ and
          $$k(s, t)
              = \frac{\sigma^2}{2\theta}
              \left(e^{-\theta|t - s|} - e^{-\theta(t + s)}\right)$$
\end{itemize}

\begin{definition}[Brownian Motion]
    $B(t):\R^+ \to \R$ is a \emph{Brownian motion} on $\R$ if\begin{enumerate}
        \item $B(0) = 0$ almost surely
        \item $B(t_0), B(t_1) - B(t_0), \dots, B(t_n - t_{n-1})$ are independent for all $t_0<t_1<t_2<\dots<t_n$
        \item $B(t + s) - B(t)\sim N(0, s)$ for $s, t \geq 0$
        \item $B(t)$ is continuous almost surely for $t>0.$
    \end{enumerate}
\end{definition}

We can generate novel Gaussian processes with a custom $m,$ and
covariance kernel $k.$ This thesis only considers symmetric
$k$ that are also stationary - that is $k(x, x^\prime)$ can be expressed as a
function of $x - x^\prime.$

\color{red}

\begin{definition}[Positive Semi-Definite Matrix]\label{def:pos_def_mat}
    An $n\times n$ matrix $\mathbf{A}$ is \emph{positive semi-definite} if
    $\mathbf{v}^T\mathbf{A}\mathbf{v} \geq 0$ for all $\mathbf{v}\in\R^n.$
\end{definition}

\begin{theorem}[Sufficient Condition for Positive Semi-Definite]
    A symmetric matrix $\mathbf{A}$ is positive semi-definite, if (and only if)
    it's eigenvalues are non-negative.
\end{theorem}
\begin{proof}
    % https://en.wikipedia.org/wiki/Definite_matrix#Eigenvalues
\end{proof}

\begin{definition}[Positive Semi-Definite Kernel]\label{def:pos_def_ker}
    A kernel $k:\mathcal{X}\times\mathcal{X}\to\R$ is
    \emph{positive semi-definite} if the matrix
    $$\mathbf{K} = \begin{bmatrix}
            k(x_1, x_1) & k(x_1, x_2) & \dots  & k(x_1, x_n) \\
            k(x_2, x_1) & \ddots      &        & \vdots      \\
            \vdots      &             & \ddots & \vdots      \\
            k(x_n, x_1) & \cdots      & \cdots & k(x_n, x_n)
        \end{bmatrix}$$
    is positive semi-definite for any collection of $x_i\in\mathcal{X}$
\end{definition}

\begin{theorem}
    All symmetric positive semi-definite matrices are covariance matrices for
    some set of random variables
\end{theorem}
\begin{proof}
    % https://www.fepress.org/wp-content/uploads/2014/06/ch7-iff-covariance_correlation_matrix.pdf
\end{proof}

\color{black}

Various classes of covariance kernel exist. The choice of covariance kernel determines how we expect the function to behave, including how `smooth' we expect the discrepency function to be. We formalise this by the notion of mean square continuity/differentiability.

\begin{definition}[Mean Square Continuous]
    A function $f:\R^d\to\R$ is \emph{mean square continuous} at $\mathbf{x}$ in the $i$th direction at if $\E(|f(\mathbf{x} + h\mathbf{e}_i) - f(\mathbf{x})|^2)\to 0$ as $|h|\to 0,$ where $\mathbf{e}_i$ is the unit vector with a 1 in the $i$th coordinate.
\end{definition}

\begin{definition}[Mean Square Differentiable]
    A function $f:\R^d\to\R$ is \emph{mean square differentiable} at $\mathbf{x}$ with derivative $f^\prime(x)$ if $\E\left[\left|\frac{f(x + h) - f(x)}{h} - f^\prime(x)\right|^2\right]\to 0$ as $|h|\to 0.$
\end{definition}

\begin{theorem}
    Brownian motion is mean square continuous, but not mean square differentiable. 
\end{theorem}
\begin{proof}
    $(B_{t + h} - B_t)^2 \sim h(Z)^2$ where $Z\sim N(0,1).$ Therefore $(B_{t + h} - B_t)^2 \sim |h|\chi_1^2 \to 0$ almost surely as $|h|\to 0,$ and hence $\E[(B_{t + h} - B_t)^2= 0]$. Since $\frac{B_{t + h} - B_t}{h} \sim N(0, 1/|h|),$ $\frac{B_{t + h} - B_t}{h}$ does not converge to any valid probability distribution as $|h| \to 0.$
\end{proof}

Some common

\color{red}

\begin{theorem}[Positive Semi-Definiteness of RBF]\label{thm:rbf_pos_def}
    The radial basis function
    $\frac{1}{\sigma^2}\exp(-\frac{(x - x^\prime)^2}{2\gamma^2})$ is a positive
    semi-definite kernel.
\end{theorem}

\begin{theorem}[Bochner's Theorem]
    Let $k$ be a stationary kernel function such that
    $k(x, x^\prime) = f(d)$. A function $k:\R^d\to\mathbb{C}$ is the covariance
    function of a weakly
    stationary mean square continuous complex-valued random process of $\R^d$
    if and only if it can be represented as
    $$k(\mathbf{\tau}) = \int_{\R^d} \exp(2\pi i \mathbf{s}\cdot\mathbf{\tau})$$
\end{theorem}

\parencite[82]{rasmussen_gaussian_2008}

\color{black}

\section{Kernel Families}

\subsection*{Mat\'{e}rn Class}

\begin{figure}
    \missingfigure{Multiple kernels}
\end{figure}

The Mat\'ern class of kernel gives flexibility over how many times mean square differentiable the realised function is.

The Mat\'ern exponential kernel is of the form
$$k_\nu(x, x^\prime)
    = \sigma^2\frac{2^{1 - \nu}}{\Gamma(\nu)}
    \left(\frac{\sqrt{2\nu}||x - x^\prime||}{\ell}\right)^\nu
    K_\nu\left(-\frac{\sqrt{2\nu}||x - x^\prime||}{\ell}\right)$$
where $K_\nu$ is a modified Bessel function (see \cite[374]{abramowitz_handbook_2013}). The general form is not very insightful, but for the most common values of $\nu = 1/2, 3/2$ and $5/2,$ we have the forms:
$$k_{1/2}(x, x^\prime)
    = \sigma^2\exp\left(-\frac{||x - x^\prime||}{\ell}\right)$$
$$k_{3/2}(x, x^\prime)
    = \sigma^2
    \left(1 + \frac{\sqrt{3}||x - x^\prime||}{\ell}\right)
    \exp\left(-\frac{\sqrt{3}||x - x^\prime||}{\ell}\right)$$
$$k_{5/2}(x, x^\prime)
    = \sigma^2
    \left(1 + \frac{\sqrt{5}||x - x^\prime||}{\ell} + \frac{5||x - x^\prime||}{3\ell^2}\right)
    \exp\left(-\frac{||x - x^\prime||^2}{2*\ell^2}\right)$$

Zero mean processes with a Mat\'ern kernel are $n$ times mean square differentiable, for all $n < \nu.$ As $\nu\to\infty,$ we get something infinitely differentiable, and specifically we get the squared exponential kernel.

\subsection*{Squared Exponential}

The squared exponential kernel is of the form
$$k(x, x^\prime)
    = \sigma^2\exp\left(-\frac{||x - x^\prime||^2}{2\ell^2}\right)$$

\subsection*{Length parameter}

\begin{figure}
    \missingfigure{different lengths on exponential kernel}
    \caption{a figure about kernel lengths}
    \label{fig:no_reg_lengths}
\end{figure}

In both of the above kernels, there is a hyperparameter $\ell,$ which we call the length parameter. This value determines how close two points need to be to be highly correlated. A small value of $\ell$ leads to a more wiggly function as seen in \ref{fig:no_reg_lengths}. 

\section{Gaussian Process Regression}

Given a choice of


\begin{figure}
    \missingfigure{Progressing GP finding}
\end{figure}


\begin{figure}
    \missingfigure{GP differing lengths and amplitudes}
\end{figure}

It asymptotically approaches the squared exponential class.

LOOK AT CHAPTER 4 SKOROKHOD STOCHASTIC I

\subsection*{Adding in observation variance}

\begin{figure}
    \missingfigure{GP differing mat\'ern $nu$s}
\end{figure}

\section{Differing mean functions}


\begin{figure}
    \missingfigure{GP mean functions}
\end{figure}