\chapter{Methods}

\section{Creation of Synthetic Data}

\begin{figure}[htbp]
    \centering
    \includegraphics[width = \textwidth]{
        ../champagne_GP_images/champagne_simulation.pdf
    }
    \caption{
        A Doob-Gillespie Simulation of the model described by
        \cite{champagne_using_2022} with $\alpha = 0.4,$ $\beta = 0.4,$
        $\gamma_L = 1 / 223,$ $\lambda = 0.04,$ $f = 1 / 72,$ $r = 1 / 60,$ and
        $\delta = 0.$ The population was 10000, with 100 initial infections
        (both blood and liver stage $I_L$).
    }
    \label{fig:champ_doob}
\end{figure}

\begin{table}[htbp]
    \caption{
        The parameters used to simulate a \textit{P. vivax} outbreak using
        the model described by \cite{champagne_using_2022}
    }
    \label{tab:synth_params}
    \centering
    \begin{tabular}{c | c | c | c}
        Parameter description                      & Parameter
                                                   & Value      & Units     \\
        \hline
        effective blood stage treatment proportion & $\alpha$
                                                   & $0.1235$      & None      \\
        effective liver stage treatment proportion & $\beta $
                                                   & $0.429$      & None      \\
        rate of liver stage disease clearance      & $\gamma_L$
                                                   & $1 / 383$  & $1/$days  \\
        rate of infection                          & $\lambda$
                                                   & $0.01$     & $1/$days  \\
        rate of relapse                            & $f$
                                                   & $1/69$     & $1/$days  \\
        rate of blood stage disease clearance      & $r$
                                                   & $1/60$     & $1/$days. \\
    \end{tabular}
\end{table}

We investigated the model by \cite{champagne_using_2022} as described in
\ref{sec:champ_mod}. A malaria epidemic was simulated using the Doob-Gillepsie
algorithm shown in Figure \ref{fig:champ_doob}, using a population
size of 10,000, and initial infected population of 100 (with both liver and blood
stage infection). The parameters used closely followed those reported in
\cite{champagne_using_2022}, with the exact parameters used reported in Table
\ref{tab:synth_params}. $\delta = 0$ was assumed to be known.
From intialisation, the simulation was run for 200,000 events
(with an event being anything that caused the size of any compartment to change
such as an infection, recovery, relapse etc.), after which, the model was
assumed to have reached steady state behaviour. A number of events was chosen
because time to convergence is highly dependent on the scales of the parameters.

\begin{table}[htbp]
    \caption{
        Observed synthetic data
        $\by^\obs := \{\iota_\text{obs}, \pi_\text{obs}, i_\text{obs}, p_{obs}\}$
        from the simulation in Figure \ref{fig:champ_doob}.
    }
    \label{tab:obs_data}
    \centering
    \begin{tabular}{c|c|c}
        Parameter Description                        & Parameter
                                                     & Observed Value     \\
        \hline
        Weekly incidence at epidemic steady state    & $\iota_\text{obs}$
                                                     & 461                \\
        Prevalence at epidemic steady state          & $\pi_\text{obs}$
                                                     & 5205               \\
        Incidence in the first month of the epidemic & $i_\text{obs}$
                                                     & 42                 \\
        Prevalence after one month of the epidemic   & $p_{obs}$
                                                     & 87                 \\
    \end{tabular}
\end{table}


New infections which instantly undergo radical cure don't change the size of
each compartment. The number of these `silent' incidences were calculated
between events using a Poisson distribution with rate
$\Delta t \times \alpha \beta \lambda (I_L + I_0) S_0 / N,$ where $\Delta t$ is
the time between events.
The observed data was taken to
be from the simulated case counts (incidence) and prevalence
(as the absolute number of people infected) of the
simulated epidemic, described in Table \ref{tab:obs_data}.

\section{Model Simulations and Discrepancy Function}

New epidemics were simulated as above, with 200,000 events and at least 30
days (to allow for calculation of incidence in the first month of the
epidemic), with parameters
$\btheta = \{\alpha, \beta, \gamma_L, \lambda, f, r\}$
For each model
$y(\btheta) = \{\iota, \pi, i, p\}$ was
calculated with the same method as $\by^\obs,$ where the intepretation of each
parameter is described in Table \ref{tab:obs_data}.

We defined the discrepancy function to be $L_2$ norm of the relative
differences
$$
    \D(\btheta) = \D(\alpha, \beta, \gamma_L, \lambda, f, r)
    := \sqrt{
        \left(\frac{\iota - \iota_\text{obs}}{\iota_\text{obs}}\right)^2
        + \left(\frac{\pi - \pi_\text{obs}}{\pi_\text{obs}}\right)^2
        + \left(\frac{i - i_\text{obs}}{i_\text{obs}}\right)^2
        + \left(\frac{p - p_\text{obs}}{p_\text{obs}}\right)^2
    }.
$$
Relative difference was chosen to limit the impact between the scale
differences of the summary statistics.

\section{Gaussian Process and Initialisation}

We approximated $\E[\ln\D(\btheta)]$ with
a Gaussian process $d_\GP(\btheta)$ surrogate model.
$d_\GP(\btheta)$ was regressed on sample means
$$
    \smtheta := \sum_{j = 1}^{30} \frac{\ln\D_j(\btheta)}{30}
$$
where the $\ln\D_j(\btheta)$s are 30 i.i.d.\ samples generated by model runs.
These were run in parallel on a supercomputer.
The samples
$\smtheta$ were assumed to be noisy observations of
$\E[\ln\D(\btheta)] + \varepsilon,$ where
$\varepsilon \sim N(0, \sigma_o^2),$ by the central limit theorem.
$\sigma_o^2$ was assumed to be independent of $\btheta,$ and has the natural
intepretation as the variance of the sample mean.
$d_\GP(\btheta)$ was assume to have unknown constant mean $m_\GP$, and kernel
$$
    k(\bm{\theta}_i, \bm{\theta}_i^\prime)
    = \sigma_k^2 (1 + z_i + \frac{z_i^2}{3})\exp(-z_i)
$$
where
$$
    z_i = \sqrt{
        5 \sum_{\theta\in \bm{\theta}}\left(
        \frac{\theta_i - \theta_i^\prime}{\ell_\theta}
        \right)^2
    }.
$$
This kernel is, a Mat\'ern kernel with $\nu = 5/2$ and automatic
relevance determination - i.e.\ each parameter $\theta\in\btheta$ was
scaled by $\ell_\theta.$ In effect, this assigns each parameter its own
length hyperparameter.

\begin{table}[htbp]
    \caption{
        Conservative upper bounds for parameters to be calibrated.
        Values were informed by
        \cite{champagne_using_2022, white_variation_2016}. All lower bounds
        were zero.
    }
    \label{tab:param_bounds}
    \centering
    \begin{tabular}{c |c |c}
        Parameter
         & Upper Bound & Unit   \\
        \hline
        Proportion of treatment clearing blood stage disease $\alpha$
         & 1           &        \\
        Proportion of treatment clearing liver stage disease $\beta$
         & 1           &        \\
        Rate of liver stage disease clearance $\gamma_L$
         & 1/30        & 1/days \\
        Rate of infection $\lambda$
         & 1/10        & 1/days \\
        Rate of relapse $f$
         & 1/14        & 1/days \\
        Rate of blood stage disease clearance $r$
         & 1/14        & 1/days
    \end{tabular}
\end{table}

All parameters to be calibrated were given conservative upper bounds after
considering values reported in the literature. $d_\GP(\btheta)$ was fit
over this compact subspace of the whole parameter space.

\begin{table}[htbp]
    \caption{
        Hyperparameters used in training $d_\GP(\btheta).$
    }
    \label{tab:hps}
    \centering
    \begin{tabular}{c|c}
        Hyperparameter    & Description                                \\
        $\sigma_o^2$      & Observation variance ($\var(\smtheta)$)    \\
        $\sigma_k^2$      & Mat\'ern kernel amplitude                  \\
        $\ell_\alpha$     & Length parameter associate with $\alpha$   \\
        $\ell_\beta$      & Length parameter associate with $\beta$    \\
        $\ell_{\gamma_L}$ & Length parameter associate with $\gamma_L$ \\
        $\ell_\lambda$    & Length parameter associate with $\lambda$  \\
        $\ell_f$          & Length parameter associate with $f$        \\
        $\ell_r$          & Length parameter associate with $r$        \\
        $m_\GP$           & Gaussian process mean
    \end{tabular}
\end{table}

Latin hypercube sampling was used to generate initialise 50 samples of the
parameter space (scaled to be between zero and the upper bounds described in
Table \ref{tab:param_bounds}). For each set of parameters,
$\smtheta$ was generated. The hyper\-parameters described in \ref{tab:hps}
were selected by leave one out cross validation, and $d_\GP^{(0)}(\btheta)$ was
fit to the samples.

\section{Bayesian Acquisition and Parameter Updates}

\begin{algorithm}
    \caption{
        Gaussian process approximation of $\smtheta$ with Bayesian updating
    }
    \label{alg:GP_reg}
    \begin{algorithmic}
        \State \textbf{Input:} Initial values for $\btheta$,
        lower and upper bounds for $\btheta$,
        initial Gaussian process model $d_\GP^{(0)}(\btheta)$
        \State \textbf{Output:} Synthetic likelihood $\hat{L}(\btheta)$

        \For{$t = 1$ to $500$}
        \State
        $\btheta^{(t)} \gets \argmin_{\btheta} \mathcal{A}_\EI(\btheta)$
        \State Sample $\smthetat$
        \If{$t\leq 6$} \Comment{Once per parameter}
        \State $j \gets t$
        \State Create $\mathbf{s}_j,$ 15 evenly spaced values
        from 0 to the upper bound of $\theta_j$ in
        Table \ref{tab:param_bounds}

        \For{$k$ in 1 to 15}
        \State $\theta^{(t)}_j \gets s_{jk}$
        \State Sample $\smthetat$
        \EndFor

        \Else
        \State $j\gets t\mod 6$ \Comment{Iterating over $\btheta$}
        \For{4 repeats}
        \State Sample $U_j\sim \mathrm{Unif}(0, m_j),$ with $m_j$
        being $\theta_j$'s upper bound
        \State $\theta^{(t)}_j \gets U_j$
        \State Sample $\smthetat$
        \EndFor
        \EndIf

        \If{$t \mod 50 == 0$} \Comment{Every 50 iterations}
        \State Reoptimize the Gaussian process hyperparameters using
        leave-one-out cross validation
        \EndIf

        \State Update $d_\GP^{(t)}(\btheta)$ on the new samples
        \EndFor
        \State \Return $d_\GP^{(500)}(\btheta)$
    \end{algorithmic}
\end{algorithm}

The Gaussian process was optimised over 500 iterations. Each iteration involved
minimising the expected improvement and obstaining a new sample $\smtheta.$
For each iteration, one of the variables $\theta_j \in \btheta$ was chosen, and
multiple samples of $\smtheta$ were taken using multiple values for $\theta_j.$
This was in order to get more improve the values for the length parameters, but
also to explore the parameter space more widely. This step is highly
parallelisable.
The hyperparameters were
reoptimised every 50 iterations, as the number of samples increased. The full
procedure is specified in Algorithm \ref{alg:GP_reg}.

The expected information was minimised using a gradient descent algorithm,
initialised at $\btheta^\dagger.$ $\btheta^\dagger$ was a
combination of
$\btheta^{**} := \argmin_{\btheta^\ast\in\bm{\Theta}^\ast} d_\GP(\btheta^\ast)$,
where $\bm{\Theta}^\ast$ is the set of previously sampled parameters, and
values between 0 and the upper bounds described in \ref{tab:param_bounds}
distributed uniformly at random. Each $\theta_j^\dagger$ was set independently,
with
$$
    \Pr(\theta_j^\dagger = \theta_j^{**})
    = 1/2
    = \Pr(\theta_j^\dagger \text{ uniformly distributed}).
$$
This was done because when $\A_\EI(\btheta)$ is very small, particularly if
$d_\GP(\btheta^\ast)$ is
large, the gradient of $\A$ can be negligible, causing the convergence criteria
to be met prematurely. Thus, random initialization is not conducive to the
algorithm's success.
Conversely, initialising at the current minimum
$\argmin_{\btheta^\ast\in\bm{\Theta}^\ast} d_\GP(\btheta^\ast)$
can limit the explored parameter space, as the current minimum may be located
near a local minimum.

$d_\GP^{(500)}(\btheta)$ is an approximation of $\E(\ln\D(\btheta)).$ Since the
variance of the sample mean $\var(\smtheta)$ is estimated by $\sigma_o^2,$ the
variance of the the log discrepency function $\ln\D(\btheta)$ is
approximately $30\sigma_o^2.$ We then used moment matching assuming that
$\D(\btheta)$ is approximately log-normally distributed
distribution, and hence can be approximated by
$$
    \hat{\D}(\btheta) \sim
    \LN\left(\E[d_\GP^{(500)}(\btheta)], 30 \sigma^2\right),
$$
where $\overset{d}{\approx}$ is approximately distributed as.
Therefore using approximate Bayesian computation described in
Algorithm \ref{alg:abc}, the probability of sampling
and accepting a $\btheta$ is
$\Pr(\btheta)\Pr(\D(\btheta) < \epsilon | \btheta),$ where
$L(\btheta):=\Pr(\D(\btheta) < \epsilon | \btheta)$ approximates the true
likelihood $\L(\btheta).$
Finally, we substitute in our approximation $\hat{\D}$ for $\D,$ to create our
synthetic likelihood
$$
    \hat{L}(\btheta) := \Pr(\hat{\D}(\btheta) < \epsilon | \btheta).
$$
Since
$$
    \ln\hat{\D}(\btheta)
    \sim N\left(\E[d_\GP^{(500)}(\btheta)], 30 \sigma^2\right),
$$
we can express $\hat{L}$ as
$$
    \hat{L}(\btheta) =\Pr(\ln\hat{\D}(\btheta) < \ln\epsilon | \btheta).
$$

The Gaussian process and Gaussian process regression was implemented using
TensorFlow \parencite{abadi_tensorflow_2015}, and all code
is available at \url{https://github.com/jaycrick/masters_project}.