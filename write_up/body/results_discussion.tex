\chapter{Results}

We outline the results of the method first by validating that each step of
the method had plausible outputs, and then by reporting the estimated
parameters based on the observed data.

\section{Validation}

\begin{table}[htbp]
    \caption{
        Final optimised Gaussian process hyperparameters
    }
    \label{tab:trained_hps}
    \centering
    \begin{tabular}{c|c}
        Hyperparameter    & Final value \\
        $\sigma_o^2$      & $0.064$      \\
        $\sigma_k^2$      & $0.762$     \\
        $\ell_\alpha$     & $0.362$     \\
        $\ell_\beta$      & $1.229$     \\
        $\ell_{\gamma_L}$ & $0.009$     \\
        $\ell_\lambda$    & $0.006$     \\
        $\ell_f$          & $0.016$     \\
        $\ell_r$          & $0.017$     \\
        $m_\GP$           & $0.881$
    \end{tabular}
\end{table}

\begin{figure}[htbp]
    \centering
    \begin{minipage}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{discreps_violin.pdf}
        \caption{Sampled $\smtheta$ violin plot}
        \label{fig:violin}
    \end{minipage}
    \begin{minipage}[b]{0.66\textwidth}
        \centering
        \begin{minipage}[b]{\textwidth}
            \centering
            \includegraphics[width=\textwidth]{hyperparam_loss_log_discrep.pdf}
            \caption[{Hyperparameter optimisation validation}]{
                A plot showing the value of the leave-one-out 
                cross-validation 
                log predictive likelihood for each gradient descent
                iteration.
            }
            \label{fig:hyper_train}
        \end{minipage}
        \begin{minipage}[b]{\textwidth}
            \centering
            \includegraphics[width=\textwidth]{initial_EI_loss_training.pdf}
            \caption[{
                Acquisition function minimisation
            }]{
                A plot showing the value of $\argmin_{\btheta}\A_\EI(\btheta)$
                for each gradient descent iteration.
            }
            \label{fig:subEI}
        \end{minipage}
    \end{minipage}
    % \caption{
    %     Validation of the results from training the Gaussian process. The
    %     \textcolor{red}{something about the discrepancy values being spread}.
    %     }
    % \label{fig:validation}
\end{figure}

The final hyperparameters are reported in Table \ref{tab:trained_hps}.
To ensure that our expected improvement and hyperparameter optimisation 
functions converged, we plotted the initial training in 
Figures \ref{fig:hyper_train} and \ref{fig:subEI}. The plots have a 
smooth curve, demonstrating that both algorithms converged at a reasonable
rate.

The violin plot of discrepancy function in Figure \ref{fig:violin}
is well dispersed suggesting a good
amount of exploration of the parameter space has been done. A large number
of samples were in areas of low mean log discrepancy, and so 
this suggests a good exploration exploitation trade-off.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{
            ../champagne_GP_images/initial_gamma_L_slice_log_discrep.pdf
        }
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{
            ../champagne_GP_images/gamma_L_slice_400_bolfi_updates_log_discrep.pdf
        }
    \end{subfigure}
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{
            ../champagne_GP_images/initial_lambda_slice_log_discrep.pdf
        }
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{
            ../champagne_GP_images/lambda_slice_400_bolfi_updates_log_discrep.pdf
        }
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{
            ../champagne_GP_images/initial_f_slice_log_discrep.pdf
        }
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{
            ../champagne_GP_images/f_slice_400_bolfi_updates_log_discrep.pdf
        }
    \end{subfigure}
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{
            ../champagne_GP_images/initial_r_slice_log_discrep.pdf
        }
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{
            ../champagne_GP_images/r_slice_400_bolfi_updates_log_discrep.pdf
        }
    \end{subfigure}
    \caption[{
        Initial and final regressed Gaussian processes for each parameter 
        except for treatment parameters
    }]{
        The left column of figures is the Gaussian process after initialisation
        $d_\GP^{(0)}(\btheta).$ The black line is $\E(d_\GP^{(0)}(\btheta)),$
        and the red lines are multiple realisations of
        $d_\GP^{(0)}(\btheta).$ The right column of figures is after 400
        sampling iterations, with the black line being
        $\E(d_\GP^{(400)}(\btheta)).$
        The blue dots are realisations of $\ln\mathcal{D}(\bm{\theta}).$
        $d_\GP$ has not been trained on these realisations.
        The parameters are varied
        univariately, with all other parameters fixed at the true parameters.
    }
    \label{fig:improving_GP}
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{
            ../champagne_GP_images/initial_alpha_slice_log_discrep.pdf
        }
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{
            ../champagne_GP_images/alpha_slice_400_bolfi_updates_log_discrep.pdf
        }
    \end{subfigure}
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{
            ../champagne_GP_images/initial_beta_slice_log_discrep.pdf
        }
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{
            ../champagne_GP_images/beta_slice_400_bolfi_updates_log_discrep.pdf
        }
    \end{subfigure}
    \caption[{
        Initial and final regressed Gaussian processes for both
        treatment parameters
    }]{
        Gaussian process approximations of the treatment parameters, as with
        Figure \ref{fig:improving_GP}
    }
    \label{fig:treatment_GP_fig}
\end{figure}

Figure \ref{fig:improving_GP} suggests that after 400 iterations
$d_\GP(\btheta)$ fits to $\E(\ln\D(\btheta))$ well.
The $\lambda$ slice has the sharpest minimum, whereas $\beta$
does not appear to have much impact on the discrepancy.
As the number of iterations increased, the Gaussian process visibly
improved at fitting to the mean. Interim iterations of $d^{(t)}_\GP(\btheta)$
can be seen in the appendix.

\section{Parameter Estimates and Synthetic Likelihoods}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{alpha_slice_synth_likelihood.pdf}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{beta_slice_synth_likelihood.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{gamma_L_slice_synth_likelihood.pdf}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{lambda_slice_synth_likelihood.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{f_slice_synth_likelihood.pdf}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{r_slice_synth_likelihood.pdf}
    \end{subfigure}%
    \caption[{
        Synthetic likelihood for all parameters
    }]{
        Final univariate synthetic likelihoods $\hat{L}(\bm{\theta})$ after
        400 sampling iterations. All values not shown were fixed at the true
        parameters.
    }
    \label{fig:final_synth_lik}
\end{figure}

\begin{table}[htbp]
    \caption[{
        Global and univariate maximum likelihood estimates for all model 
        parameters
    }]{
        Estimates of our model parameters.
        The maximum likelihood estimate (MLE) of the true parameters using
        $\hat{L}.$ The maximum slice estimate was the one-dimensional
        maximum likelihood estimate where all other parameters are held
        constant at the true value.
    }
    \label{tab:param_est}
    \centering
    \begin{tabular}{c | c | c | c}
        Parameter  & True     & MLE     & ML Slice Estimate \\
        \hline
        $\alpha$   & $0.124$  & $0.049$ & $0.02$            \\
        $\beta $   & $0.43$   & $0.42$  & $0.64$            \\
        $\gamma_L$ & $0.0026$ & $0.006$ & $0.005$           \\
        $\lambda$  & $0.01$   & $0.01$  & $0.01$            \\
        $f$        & $0.014$  & $0.018$ & $0.017$           \\
        $r$        & $0.017$  & $0.024$ & $0.022$           \\
    \end{tabular}
\end{table}

For parameter estimation, we typically look at likelihood instead of 
discrepancy. This is done by inverting 
that distribution function (see Equation \ref{eq:fin_lik}). 
In Table \ref{tab:param_est} we present the global maximum 
likelihood estimate found by finding the maximum synthetic likelihood 
$\argmax_{\btheta}\hat{L}(\btheta).$

In Figure \ref{fig:final_synth_lik} we show our final likelihood function 
estimate from the trained Gaussian process.
For simplicity we only present the likelihood across parameter slices, where 
the parameter of 
interest is allowed to vary, while all other parameters are held constant at
their true values. The maximum likelihood values for each parameter slice
are also presented in Table \ref{tab:param_est} but do are very similar to the
maximum likelihood estimates and the true values. For this reason, 
we can be confident that our synthetic likelihood is a good approximation of
the true likelihood, despite the true likelihood being infeasible to compare
to. Certain parameters such as $\lambda,$ $r,$ and $\gamma_L$ have sharper 
likelihood peaks than parameters such as $\alpha$ and $\beta,$ suggesting the
data we used may lend itself to estimating parameters such as $\lambda$ over
the treatment parameters, although all likelihood slices calculated were
unimodal.

\chapter{Discussion}

To our knowledge, the synthetic likelihood described above
has not been used to calibrate a malaria model before.
\Citeauthor{champagne_using_2022} calibrate the model we use
by finding the ODE equilibrium and fitting a single parameter $\lambda$ to
incidence data. 
Note that this comes with limitations. For example, it relies on data being
observed when the disease is roughly at equilibrum in the population.
This is not desirable for modelling outbreaks or seasonality effects.
\Citeauthor{champagne_using_2022}'s approach to parameter calibration 
does not work in these scenarios.

In comparison, not only were we able to effectively
recover the true $\lambda$ from a synthetic run, we were able to recover
all model parameters simultaneously.

This methodology is very robust to both frequentist and Bayesian inference.
Under a Bayesian framework,
since we can evaluate the synthetic likelihood for any $\btheta,$
we can use a Metropolis Hasting sampler to obtain samples from a distribution
approximately equal to the posterior distribution $\Pr(\btheta | \by^\obs).$
Alternatively, standard frequentist inference can also be used on our
synthetic likelihood $\hat{L}.$ For example, numerically approximating the
observed Fisher information matrix
$$
    F(\hat{\theta})
    = -\frac{\partial \ln\hat{L}}{\partial \btheta\partial \btheta^T}(\hat{\btheta})
$$
allows us to do hypothesis testing and construct confidence intervals
since asymptotically $\hat{\btheta} \sim N(\btheta, F^{-1}(\hat{\btheta}))$
(see \cite{fahrmeir_multivariate_2013}).

Although the likelihood-free procedure we outlined for this model closely
resembles \cite{gutmann_bayesian_2016}, a few significant changes
improve on the method outlined in that manuscript.

The most significant change is the choice to model the sample mean as a noisy
Gaussian process rather than modelling the discrepancy function as a noisy
Gaussian process. 
Changing this assumption means that we are not subject to the distributional 
assumption of normal noise and constant variance 
with respect to $\D(\btheta).$ Once we have an approximation for the mean
as a function of $\btheta,$ we could choose a distribution that scales
with the mean. This is analogous to the generalised linear modelling framework.
For example, assuming that $\D(\btheta)$ follows a
Gamma distribution may be reasonable. 
Letting $\mu(\btheta) := \E[\D(\btheta)],$ we can
approximate $\D(\btheta)$ with
$
    \hat{\D}(\btheta)
    \sim \mathrm{Gamma}\left(\frac{\mu(\btheta)}{\phi}, \frac{1}{\phi}\right),
$
where $\frac{\mu(\btheta)}{\phi}, \frac{1}{\phi}$ are the shape and rate
parameters. Trivially,
$$
    \E[\hat{\D}(\btheta)]
    = \frac{\mu(\btheta)}{\phi}/\frac{1}{\phi} = \mu(\btheta),
$$
and for a fixed $\phi,$ $\var[\D(\btheta)] = \phi\mu(\btheta),$ so the variance
scales linearly with the mean. If this behaviour is observed
empirically, then such a choice will be preferable since then
$\hat{L}(\btheta)$ will better approximate the
true likelihood. This can be done with any individual parameter distribution
with
a fixed variance structure.

Alternatively, the sample variance could be modelled with a different
Gaussian process $s^2_\GP(\btheta)$ along with the sample mean.
Any two-parameter distribution could be moment matched by
the two Gaussian processes to get a more accurate $\hat{L}(\btheta).$
Therefore, if empirically we observe that $\D(\btheta)$ is approximately Gamma
distributed, then we could approximate $\D(\btheta)$ with
$$
    \hat{\D}(\btheta)
    \sim \mathrm{Gamma}\left(
    \frac{\mu^2(\btheta)}{\sigma^2(\btheta)},
    \frac{\mu(\btheta)}{\sigma^2(\btheta)}
    \right),
$$ where $\var[\hat{\D}(\btheta)] = \sigma^2(\btheta)$ and
$\E(\hat{\D}(\btheta)) =\mu(\btheta)$ as required.

The mean and variance do not have a linked structure in the discrepancy function
we used for the Champagne model.
This can be seen particularly in the $\lambda$ slice in Figure
\ref{fig:improving_GP}.
For $\btheta$ with $\lambda < 0.03,$ $\var(\ln\D(\btheta))$, is small, and
$\E(\ln\D(\theta))\approx 0$. However, for $\lambda \approx 0.02,$ we also have
$\E[\ln\D(\theta)] \approx 0,$ but the variance is observably larger. Therefore,
if we had modelled the sample variance of our log discrepancy as
a noisy Gaussian process $s^2_\GP(\btheta),$ then we could have approximated
$\D(\btheta)$ with
$\hat{\D}(\theta) \sim \LN\left(\E(d_\GP(\btheta)), \sigma^2(\btheta)\right),$
where $\mu(\btheta) := \E(d_\GP(\btheta))$ and
$\sigma^2(\btheta) := \E(s^2_\GP(\btheta)).$

Empirically, it is not surprising that the variance is not constant across the
parameter space or even mean dependent. Disease model behaviour is heavily
dependent on the values of the parameters. For example, around bifurcation points,
a slight change in parameters may lead to a disease model that dies out 
sometimes but reaches equilibrium in other runs. Here, we expect
$\var[\D(\btheta)]$ to be large. This threshold is important in disease 
modelling and is called $R_0:$ the expected number of secondary cases a single
infectious individual causes in a completely susceptible population.
But when $\btheta$ is changed only a small
amount such that $R_0<1,$ and so 
the disease consistently dies out, the $\var[\D(\btheta)]$
will be close to $0.$ Since the model
run will always end with a disease-free population, the summary statistics,
such as incidence or prevalence, will always be $0.$ This is likely what
happens for very small $\lambda$ in Figure \ref{fig:improving_GP}.

This highlights another problem. Around bifurcation points, such as near where
$R_0 = 1$, $\E(\D(\btheta))$ is expected to behave 
erratically. \Citeauthor{gutmann_bayesian_2016} use a
squared exponential kernel for their Gaussian process approximation, which
cannot capture this behaviour without
making any length scales very large.
On the other hand, a Mat\'ern 5/2 kernel, as we used, is still smooth but allows
enough flexibility that sharp changes in the target function do not require
overcompensation in the length scales.
For an example demonstrating the utility of the Mat\'ern
kernel over the squared exponential in the case of a non-smooth function,
see \cite{jones_matern_2021}.
Another possible solution is to use a Student-$t$ process to approximate the
discrepancy, as it has heavier tails, so it is more forgiving to sudden jumps.
The multivariate Student-$t$ distribution has some properties analogous to
the multivariate normal distribution. This includes an analytic solution
to the conditional distribution, similar to Theorem \ref{thm:cond_mvn}; for
more details, see \cite{shah_studentt_2014}.

\cite{gutmann_bayesian_2016} use the lower confidence bound acquisition
function, where the
exploration parameter is the slowly increasing function
$$\eta_t:= \sqrt{2\ln\left(\frac{t^{2/d + 2}\pi^2}{3\varepsilon}\right)}.$$
This is chosen because, under the squared exponential kernel and compact
support, the lower confidence bound samples are shown to be no regret with high
probability. There are multiple issues with this. The first is that
\Citeauthor{gutmann_bayesian_2016} seem to have inherited this form from
\cite{brochu_tutorial_2010}. However, the citation in
\cite{brochu_tutorial_2010} wrongly reproduces the result in
\cite{srinivas_gaussian_2010},
\footnote{
    One Python package that implements the methodology in
    \cite{gutmann_bayesian_2016} notes this error; see
    \url{
        https://github.com/elfi-dev/elfi/blob/dev/elfi/methods/bo/acquisition.py
    }
} which should be
$$\eta_t:= \sqrt{2\ln\left(\frac{t^{2d + 2}\pi^2}{3\varepsilon}\right)}.$$
When we tried these exploration parameters, the choice of
$\varepsilon$ between $(0, 1)$ largely led to repeated sampling from the same
set of parameters, even for very small $\varepsilon.$ This is similar to the
behaviour reported in \cite{gutmann_bayesian_2016}. Secondly,
\Citeauthor{gelman_bayesian_2014} do not restrict the parameter space to
a compact subset of the space, so theoretical guarantees of no regret are
not valid. Finally, \Citeauthor{srinivas_gaussian_2010} find this assuming
zero mean Gaussian processes. \Citeauthor{gutmann_bayesian_2016} assume
a quadratic mean prior.

Even though we consider a compact subset of the parameter space,
we did not use a zero-mean Gaussian process, and we used the Mat\'ern kernel,
Therefore, we did not want to use this formulation of the lower confidence bound,
and instead chose expected improvement.

The quadratic mean assumption in \cite{gutmann_bayesian_2016} is also
problematic. Suppose the mean of the Gaussian process
is trained on a set of concave data near a local
minimum. In that case, 
no matter which acquisition function is chosen, areas away from the
local minimum may not be sampled since the mean function will dominate
the predicted behaviour of $\D(\btheta),$ so exploration will be minimal.
This also may explain the behaviour of the acquisition function sampling close
to the same point repeatedly.

Although we have validated this method on a relatively low dimensional
$\btheta,$ for models with high dimensionality,
the construction
of a synthetic likelihood will likely have greater comparative benefits
than other methods, such as approximate Bayesian computation.
This is because as the dimensionality increases, the curse of dimensionality
means that randomly drawn points will be increasingly further away on average.
Therefore, many more samples from the prior distribution function are likely to
produce $\D(\btheta) > \epsilon,$ particularly if $\D(\btheta)$ is small in
a small region. Optimising the acquisition function each time encourages
efficient sampling from areas likely to be beneficial to sample from.

One way to reduce the computational overhead of this method would be to reduce
the number of samples used to calculate the sample mean. Our method used
30 samples to ensure convergence to the normal distribution. However, less 
could be
taken with the trade-off of a larger observation variance. The sample mean
can be calculated in parallel, so the rate-limiting step minimises the
acquisition function in each iteration. Rather than uniformly sampling
a single parameter, multivariate noise could be added to
$\btheta$ to sample multiple sample means simultaneously.
Resources should be maximally allocated
to calculate as many $\D(\btheta)$ as feasible in one step, split between
multiple repeats for the sample mean, and multiple $\btheta$s for better
training of the Gaussian process approximation.